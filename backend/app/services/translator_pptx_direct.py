#!/usr/bin/env python3
"""
translator_pptx_direct.py

å¹¶è¡Œä¼˜åŒ–ç‰ˆ PPTX ç¿»è¯‘å™¨ï¼ˆåˆå¹¶ runã€æ— è°ƒè¯•æ ‡è®°ï¼‰

Usage:
    python translator_pptx_direct.py input.pptx output.pptx src_lang tgt_lang [engine]

ä¾èµ–:
    pip install python-pptx
    å¹¶ç¡®ä¿é¡¹ç›®å†…æœ‰ translate_batch(texts, src, tgt, engine=...) å¯è°ƒç”¨
"""
from pptx import Presentation
from pptx.util import Pt
from pptx.dml.color import RGBColor
from pptx.enum.shapes import MSO_SHAPE_TYPE
import sys
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict, Any
import traceback

# è°ƒæ•´ä¸‹é¢å¯¼å…¥ä¸ºä½ é¡¹ç›®ä¸­ utils_translator çš„ä½ç½®
try:
    # prefer package import if running inside app module
    from app.services.utils_translator import translate_batch
    from app.services.terminology_service import (
        get_terminology_options,
        preprocess_texts,
        preprocess_texts_with_categories,
        postprocess_texts,
    )
    from app.database import SessionLocal
except Exception:
    # fallback: assume utils_translator.py is in same folder
    try:
        from utils_translator import translate_batch
    except Exception:
        raise ImportError(
            "æ— æ³•å¯¼å…¥ translate_batchã€‚è¯·ç¡®ä¿ app.services.utils_translator.translate_batch å¯ç”¨ï¼Œ"
            "æˆ–è€…æŠŠ utils_translator.py æ”¾åˆ°åŒçº§ç›®å½•ã€‚"
        )


def is_translatable(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦ç¿»è¯‘ï¼šç©º/çº¯æ•°å­—/çº¯ç¬¦å·/è¿‡çŸ­æ–‡æœ¬è·³è¿‡"""
    if text is None:
        return False
    s = text.strip()
    if not s:
        return False
    if s.isdigit():
        return False
    # å¦‚æœéƒ½æ˜¯æ ‡ç‚¹/ç¬¦å·ï¼ˆæ²¡æœ‰å­—æ¯æˆ–æ±‰å­—ç­‰ï¼‰ï¼Œè·³è¿‡
    if all(not ch.isalnum() for ch in s):
        return False
    # è¿‡æ»¤æ‰è¿‡çŸ­çš„æ–‡æœ¬ï¼ˆå°‘äº2ä¸ªå­—ç¬¦ï¼‰ï¼Œé¿å…ç¿»è¯‘å¤±è´¥
    if len(s) < 2:
        return False
    # è¿‡æ»¤æ‰åªåŒ…å«å•ä¸ªå­—ç¬¦çš„æ–‡æœ¬
    if len(s) == 1 and not s.isalnum():
        return False
    return True


def chunk_list(lst: List[Any], n_chunks: int) -> List[List[Any]]:
    """æŠŠ list å¹³å‡åˆ‡åˆ†æˆ n_chunks ç‰‡ï¼ˆå°½é‡å‡åŒ€ï¼‰"""
    if n_chunks <= 0:
        return [lst]
    k = max(1, math.ceil(len(lst) / n_chunks))
    return [lst[i:i + k] for i in range(0, len(lst), k)]


def batch_translate_parallel(texts: List[str], src: str, tgt: str, engine: str = "deepseek", max_workers: int = 4, **options) -> List[str]:
    """
    å°† texts åˆ‡å—åå¹¶è¡Œè°ƒç”¨ translate_batchï¼Œå¯¹æ¯ä¸ªå—è¿”å›ç¿»è¯‘ç»“æœï¼ˆä¿æŒ orderï¼‰
    å¦‚æœæŸä¸ªå—æŠ›å¼‚å¸¸ï¼Œä¼šåœ¨ä¸»çº¿ç¨‹ä¸­å°è¯•äºŒåˆ†é‡è¯•ï¼ˆé€’å½’ï¼‰ã€‚
    """
    if not texts:
        return []

    # small optimization: if only one text, call directly and unwrap result
    if len(texts) == 1:
        _res = translate_batch(texts, src, tgt, engine=engine, **options)
        if isinstance(_res, tuple) and len(_res) >= 1:
            _res = _res[0]
        # ensure list[str]
        if isinstance(_res, list):
            return [str(_res[0])] if _res else [texts[0]]
        return [str(_res)]

    # choose number of chunks by length and max_workers
    n_chunks = min(max_workers, max(1, len(texts) // 8))  # æ¯ chunk æœ€å°‘åŒ…å« ~8 ä¸ªå¥å­ä»¥å‡å°‘è¯·æ±‚æ•°
    chunks = chunk_list(texts, n_chunks)

    results = [None] * len(texts)
    index_offsets = []
    offset = 0
    for c in chunks:
        index_offsets.append(offset)
        offset += len(c)

    # ThreadPoolExecutor calling translate_batch for each chunk
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        future_to_chunk_idx = {}
        for idx, chunk in enumerate(chunks):
            future = ex.submit(translate_batch, chunk, src, tgt, engine, **options)
            future_to_chunk_idx[future] = idx

        for fut in as_completed(future_to_chunk_idx):
            idx = future_to_chunk_idx[fut]
            offset = index_offsets[idx]
            try:
                translated_chunk = fut.result()
                # unwrap tuple (texts, tokens) if necessary
                if isinstance(translated_chunk, tuple) and len(translated_chunk) >= 1:
                    translated_chunk = translated_chunk[0]
                # safety: ensure returned length matches
                if not isinstance(translated_chunk, list) or len(translated_chunk) != len(chunks[idx]):
                    # fallback to single-item translate for that chunk
                    translated_chunk = []
                    for item in chunks[idx]:
                        _single = translate_batch([item], src, tgt, engine, **options)
                        if isinstance(_single, tuple) and len(_single) >= 1:
                            _single = _single[0]
                        if isinstance(_single, list) and len(_single) >= 1:
                            translated_chunk.append(str(_single[0]))
                        else:
                            translated_chunk.append(item)
                for i, t in enumerate(translated_chunk):
                    results[offset + i] = str(t) if t is not None else texts[offset + i]
            except Exception as e:
                # å¼‚å¸¸ï¼šå¯¹è¯¥ chunk è¿›è¡ŒäºŒåˆ†é‡è¯•
                try:
                    left = chunks[idx][:len(chunks[idx]) // 2]
                    right = chunks[idx][len(chunks[idx]) // 2:]
                    left_tr = batch_translate_parallel(left, src, tgt, engine, max_workers=max_workers)
                    right_tr = batch_translate_parallel(right, src, tgt, engine, max_workers=max_workers)
                    translated_chunk = left_tr + right_tr
                    for i, t in enumerate(translated_chunk):
                        results[offset + i] = t
                except Exception:
                    # æœ€åé€€åŒ–ï¼šé€æ¡è°ƒç”¨ translate_batch
                    for i, item in enumerate(chunks[idx]):
                        single_tr = translate_batch([item], src, tgt, engine, **options)
                        if isinstance(single_tr, tuple) and len(single_tr) >= 1:
                            single_tr = single_tr[0]
                        if isinstance(single_tr, list) and len(single_tr) >= 1:
                            results[offset + i] = str(single_tr[0])
                        else:
                            results[offset + i] = item

    # final safety: replace any None by original texts
    for i in range(len(results)):
        if results[i] is None:
            results[i] = texts[i]
    return results


def collect_text_items(prs: Presentation) -> Tuple[List[Tuple[str, int, int, int]], List[str]]:
    """
    éå† PPTX æå–å¯ç¿»è¯‘æ–‡æœ¬æ®µï¼ˆåˆå¹¶ run åçš„æ®µè½ï¼‰
    è¿”å›:
      items: list of (part_key, slide_idx, shape_idx, para_idx)
             part_key ç”¨äºåœ¨åç»­å†™å›æ—¶å®šä½ï¼ˆè¿™é‡Œæˆ‘ä»¬ç”¨ tuple è½¬ä¸º strï¼‰
      texts: list of paragraph texts (same order)
    è¯´æ˜ï¼špart_key åªæ˜¯ä¸ªæ ‡è¯†ï¼Œå†™å›æ—¶é‡‡ç”¨ç›¸åŒçš„éå†é¡ºåºæ¥å†™å›ï¼ˆä¿è¯ä¸€ä¸€å¯¹åº”ï¼‰
    """
    items = []
    texts = []

    def iter_shapes_recursive(container):
        for shp in container.shapes:
            yield shp
            # é€’å½’å¤„ç†ç»„åˆå½¢çŠ¶
            try:
                if shp.shape_type == MSO_SHAPE_TYPE.GROUP and hasattr(shp, 'shapes'):
                    for inner in iter_shapes_recursive(shp):
                        yield inner
            except Exception:
                pass

    for s_idx, slide in enumerate(prs.slides):
        for sh_idx, shape in enumerate(list(iter_shapes_recursive(slide))):
            # text frame (æ™®é€šæ–‡æœ¬æ¡†/å ä½/æ ‡é¢˜)
            if shape.has_text_frame:
                tf = shape.text_frame
                for p_idx, para in enumerate(tf.paragraphs):
                    merged = "".join(run.text or "" for run in para.runs)
                    if is_translatable(merged):
                        items.append((s_idx, sh_idx, p_idx, "text_frame"))
                        texts.append(merged)
            # table cell
            elif shape.has_table:
                table = shape.table
                for r in range(len(table.rows)):
                    for c in range(len(table.columns)):
                        cell = table.cell(r, c)
                        # treat each paragraph in cell
                        for p_idx, para in enumerate(cell.text_frame.paragraphs):
                            merged = "".join(run.text or "" for run in para.runs)
                            if is_translatable(merged):
                                # encode cell position in item
                                items.append((s_idx, sh_idx, (r, c, p_idx), "table_cell"))
                                texts.append(merged)

        # slide notes
        if slide.has_notes_slide:
            notes = slide.notes_slide
            for sh_idx, nshape in enumerate(notes.shapes):
                if nshape.has_text_frame:
                    for p_idx, para in enumerate(nshape.text_frame.paragraphs):
                        merged = "".join(run.text or "" for run in para.runs)
                        if is_translatable(merged):
                            items.append((s_idx, sh_idx, p_idx, "notes"))
                            texts.append(merged)

    return items, texts


def write_translations_back(prs: Presentation, items: List[Tuple], original_texts: List[str], translated_map: Dict[str, str]) -> None:
    """
    æ ¹æ® itemsï¼ˆä¸ texts åŒåºï¼‰æŠŠ translations å†™å›åˆ° prs ä¸­ã€‚
    å†™å›ç­–ç•¥ï¼š
      - é’ˆå¯¹æ¯ä¸ªæ®µè½ï¼Œå…ˆæ¸…é™¤è¯¥æ®µè½æ‰€æœ‰ runï¼Œç„¶åæ·»åŠ ä¸€ä¸ª run æ”¾å…¥è¯‘æ–‡
      - ä¿ç•™ç¬¬ä¸€ä¸ª run çš„æ ·å¼ï¼ˆè‹¥å­˜åœ¨ï¼‰ä½œä¸ºå¤§è‡´æ ·å¼
      - å¯¹è¡¨æ ¼å•å…ƒæ ¼ä¸ notes åšç›¸åŒå¤„ç†
    """
    for idx, item in enumerate(items):
        s_idx, sh_idx, p_info, kind = item
        translated = translated_map.get(original_texts[idx], original_texts[idx])

        slide = prs.slides[s_idx]
        # bounds checking
        if sh_idx >= len(slide.shapes):
            continue
        shape = slide.shapes[sh_idx]

        if kind == "text_frame":
            if not shape.has_text_frame:
                continue
            para_idx = p_info
            tf = shape.text_frame
            if para_idx >= len(tf.paragraphs):
                continue
            para = tf.paragraphs[para_idx]
            # capture style from first run if exists
            template_style = {}
            if para.runs and len(para.runs) > 0:
                r0 = para.runs[0]
                template_style = {
                    "font_name": r0.font.name,
                    "size": r0.font.size,
                    "bold": r0.font.bold,
                    "italic": r0.font.italic,
                    "underline": r0.font.underline,
                    "color": getattr(r0.font.color, 'rgb', None)
                }
            # clear paragraph's runs by setting text to ''
            # paragraph has no clear() API, but we can replace runs by setting text of first run and removing others
            if para.runs:
                # set first run to translated text
                para.runs[0].text = translated
                # apply template style if present
                r = para.runs[0]
                apply_run_style_from_dict(r, template_style)
                # remove extra runs
                for _ in range(len(para.runs) - 1):
                    # remove last run
                    try:
                        last = para.runs[-1]
                        # there's no direct remove; set text to '' to effectively clear it
                        last.text = ""
                    except Exception:
                        pass
            else:
                # no run exists? add one
                run = para.add_run()
                run.text = translated

        elif kind == "table_cell":
            # p_info = (row, col, para_idx)
            r_idx, c_idx, para_idx = p_info
            if not shape.has_table:
                continue
            table = shape.table
            if r_idx >= len(table.rows) or c_idx >= len(table.columns):
                continue
            cell = table.cell(r_idx, c_idx)
            tf = cell.text_frame
            if para_idx >= len(tf.paragraphs):
                continue
            para = tf.paragraphs[para_idx]
            template_style = {}
            if para.runs and len(para.runs) > 0:
                r0 = para.runs[0]
                template_style = {
                    "font_name": r0.font.name,
                    "size": r0.font.size,
                    "bold": r0.font.bold,
                    "italic": r0.font.italic,
                    "underline": r0.font.underline,
                    "color": getattr(r0.font.color, 'rgb', None)
                }
            if para.runs:
                para.runs[0].text = translated
                apply_run_style_from_dict(para.runs[0], template_style)
                for _ in range(len(para.runs) - 1):
                    try:
                        para.runs[-1].text = ""
                    except Exception:
                        pass
            else:
                run = para.add_run()
                run.text = translated

        elif kind == "notes":
            # p_info is paragraph index in notes slide shapes; we used sh_idx to find a notes shape index earlier
            para_idx = p_info
            notes = slide.notes_slide
            # find the shape at sh_idx (may be different index if shapes changed)
            if sh_idx >= len(notes.shapes):
                # fallback: try to find any text shape and use para_idx-th paragraph
                for nsh in notes.shapes:
                    if nsh.has_text_frame:
                        tf = nsh.text_frame
                        if para_idx < len(tf.paragraphs):
                            para = tf.paragraphs[para_idx]
                            if para.runs:
                                para.runs[0].text = translated
                                apply_run_style_from_dict(para.runs[0], {})
                                for _ in range(len(para.runs) - 1):
                                    para.runs[-1].text = ""
                            else:
                                run = para.add_run()
                                run.text = translated
                            break
                continue
            nshape = notes.shapes[sh_idx]
            if not nshape.has_text_frame:
                continue
            tf = nshape.text_frame
            if para_idx >= len(tf.paragraphs):
                continue
            para = tf.paragraphs[para_idx]
            template_style = {}
            if para.runs and len(para.runs) > 0:
                r0 = para.runs[0]
                template_style = {
                    "font_name": r0.font.name,
                    "size": r0.font.size,
                    "bold": r0.font.bold,
                    "italic": r0.font.italic,
                    "underline": r0.font.underline,
                    "color": getattr(r0.font.color, 'rgb', None)
                }
            if para.runs:
                para.runs[0].text = translated
                apply_run_style_from_dict(para.runs[0], template_style)
                for _ in range(len(para.runs) - 1):
                    try:
                        para.runs[-1].text = ""
                    except Exception:
                        pass
            else:
                run = para.add_run()
                run.text = translated


def apply_run_style_from_dict(run, style: Dict[str, Any]):
    """æŠŠ style åº”ç”¨åˆ° run.font"""
    font = run.font
    if not style:
        return
    if style.get("font_name"):
        try:
            font.name = style["font_name"]
        except Exception:
            pass
    if style.get("size"):
        try:
            font.size = style["size"]
        except Exception:
            pass
    if style.get("bold") is not None:
        font.bold = style["bold"]
    if style.get("italic") is not None:
        font.italic = style["italic"]
    if style.get("underline") is not None:
        font.underline = style["underline"]
    if style.get("color") is not None:
        try:
            # color is RGBColor
            if isinstance(style["color"], RGBColor):
                font.color.rgb = style["color"]
            else:
                # if it's tuple or hex string, try to set flexibly
                font.color.rgb = style["color"]
        except Exception:
            pass


def translate_pptx(input_path: str, output_path: str, src: str, tgt: str, engine: str = "deepseek", max_workers: int = 4, user_id: int | None = None, **kwargs):
    prs = Presentation(input_path)

    items, texts = collect_text_items(prs)
    if not texts:
        prs.save(output_path)
        return {"token_count": 0, "character_count": 0}

    total_character_count = sum(len(t) for t in texts)

    # å»é‡ï¼šä¿æŒ items/texts é¡ºåºï¼Œä½†åˆ›å»º unique list ä¸æ˜ å°„
    unique_texts = []
    seen = {}
    for t in texts:
        if t not in seen:
            seen[t] = len(unique_texts)
            unique_texts.append(t)

    # å¹¶è¡Œæ‰¹é‡ç¿»è¯‘ unique_texts
    # æœ¯è¯­å‰/åå¤„ç†
    try:
        db = SessionLocal()
        options = get_terminology_options(db)
        if options.get("terminology_enabled", True):
            processed_texts, mappings = preprocess_texts(db, unique_texts, src, tgt, case_sensitive=bool(options.get("case_sensitive", False)), user_id=user_id)
        else:
            processed_texts, mappings = unique_texts, [{} for _ in unique_texts]
    finally:
        try:
            db.close()
        except Exception:
            pass

    # Qwen3: é¿å…å¹¶è¡Œï¼Œæ”¹ä¸ºé¡ºåºä»¥é™ä½ 429 è§¦å‘ç‡
    if engine and str(engine).lower() == 'qwen3':
        # é¡ºåºè°ƒç”¨é€æ¡ç¿»è¯‘ï¼Œå¹¶åŠ å…¥è½»å¾®èŠ‚æµï¼Œå‡å°‘429
        translated_unique = []
        for s in processed_texts:
            _r = translate_batch([s], src, tgt, engine=engine)
            if isinstance(_r, tuple) and len(_r) >= 1:
                _r = _r[0]
            if isinstance(_r, list) and _r:
                translated_unique.append(str(_r[0]))
            else:
                translated_unique.append(s)
            try:
                import time as _t
                _t.sleep(0.05)
            except Exception:
                pass
    else:
        # å¹¶è¡Œæ‰¹é‡ç¿»è¯‘ï¼Œæå‡å¤§æ–‡æ¡£é€Ÿåº¦
        translated_unique = batch_translate_parallel(processed_texts, src, tgt, engine=engine, max_workers=max_workers)
    total_token_count = 0

    if options.get("terminology_enabled", True):
        translated_unique = postprocess_texts(translated_unique, mappings)

    # build map original -> translated
    translated_map = {u: translated_unique[i] for i, u in enumerate(unique_texts)}

    # å†™å›æ‰€æœ‰æ®µè½ï¼ˆæŒ‰åŸå§‹ items é¡ºåºï¼‰
    write_translations_back(prs, items, texts, translated_map)

    # ä¿å­˜
    prs.save(output_path)
    
    return {
        "token_count": total_token_count,
        "character_count": total_character_count
    }

import pptx
from ..services.utils_translator import translate_batch
from .translator_pptx_ooxml import translate_pptx_ooxml

def is_translatable(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦éœ€è¦ç¿»è¯‘ï¼šéç©ºã€éçº¯æ•°å­—ã€éçº¯ç¬¦å·"""
    if not text or not text.strip():
        return False
    stripped = text.strip()
    if stripped.isdigit():
        return False
    if all(not ch.isalnum() for ch in stripped):
        return False
    return True

def _normalize_text(s: str) -> str:
    if not isinstance(s, str):
        return s
    # æ¸…ç†æ§åˆ¶å­—ç¬¦æ ‡è®°ï¼Œå¸¸è§ _x000B_, _x000D_ ç­‰ï¼›æ›¿æ¢ä¸ºæ¢è¡Œæˆ–ç©º
    s = s.replace("_x000B_", "\n").replace("_x000b_", "\n")
    s = s.replace("_x000D_", "\n").replace("_x000d_", "\n")
    # ä¹Ÿæ¸…ç†çœŸå®çš„ VT æ§åˆ¶ç¬¦
    s = s.replace("\x0b", "\n")
    return s


def _iter_shapes_recursive(container):
    for shp in container.shapes:
        yield shp
        try:
            if shp.shape_type == MSO_SHAPE_TYPE.GROUP and hasattr(shp, 'shapes'):
                for inner in _iter_shapes_recursive(shp):
                    yield inner
        except Exception:
            pass


def translate_pptx_direct(input_path, output_path, src_lang, tgt_lang, engine="deepseek", user_id: int | None = None, **kwargs):
    # ä¼˜å…ˆä½¿ç”¨ OOXML å±‚æ›¿æ¢ï¼Œä»…æ”¹ a:t æ–‡æœ¬ï¼Œæœ€å¤§åŒ–ä¿ç•™æ ·å¼/å¸ƒå±€
    try:
        cat_ids = kwargs.get('category_ids')
        return translate_pptx_ooxml(input_path, output_path, src_lang, tgt_lang, engine=engine, user_id=user_id, category_ids=cat_ids)
    except Exception:
        # å‡ºé”™æ—¶å›é€€åˆ° python-pptx æ”¹å†™æ–¹æ¡ˆ
        pass
    """Translate a PPTX file directly, preserving formatting and attempting layout adjustments."""
    from pptx import Presentation
    
    # Load the presentation
    prs = Presentation(input_path)
    total_token_count = 0
    total_character_count = 0
    
    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬å’Œä½ç½®ä¿¡æ¯
    text_items = []  # [(slide_idx, shape_idx, paragraph_idx, text, is_table, row_idx, col_idx), ...]
    
    for slide_idx, slide in enumerate(prs.slides):
        # é€’å½’éå†å½¢çŠ¶ï¼ˆæ”¯æŒç»„åˆï¼‰
        for shape_idx, shape in enumerate(list(_iter_shapes_recursive(slide))):
            if hasattr(shape, "text_frame") and shape.text_frame:
                for para_idx, paragraph in enumerate(shape.text_frame.paragraphs):
                    original_text = _normalize_text(paragraph.text)
                    if is_translatable(original_text):
                        text_items.append({
                            'type': 'shape',
                            'slide_idx': slide_idx,
                            'shape_idx': shape_idx,
                            'para_idx': para_idx,
                            'text': original_text,
                            'paragraph': paragraph
                        })
                        total_character_count += len(original_text)

            # Collect text from tables
            if shape.has_table:
                for row_idx, row in enumerate(shape.table.rows):
                    for col_idx, cell in enumerate(row.cells):
                        for para_idx, paragraph in enumerate(cell.text_frame.paragraphs):
                            original_text = _normalize_text(paragraph.text)
                            if is_translatable(original_text):
                                text_items.append({
                                    'type': 'table',
                                    'slide_idx': slide_idx,
                                    'shape_idx': shape_idx,
                                    'row_idx': row_idx,
                                    'col_idx': col_idx,
                                    'para_idx': para_idx,
                                    'text': original_text,
                                    'paragraph': paragraph
                                })
                                total_character_count += len(original_text)
        
        # Collect text from slide notes
        if slide.has_notes_slide:
            notes_slide = slide.notes_slide
            if notes_slide.notes_text_frame:
                for para_idx, paragraph in enumerate(notes_slide.notes_text_frame.paragraphs):
                    original_text = _normalize_text(paragraph.text)
                    if is_translatable(original_text):
                        text_items.append({
                            'type': 'notes',
                            'slide_idx': slide_idx,
                            'para_idx': para_idx,
                            'text': original_text,
                            'paragraph': paragraph
                        })
                        total_character_count += len(original_text)
    
    print(f"æ”¶é›†åˆ° {len(text_items)} ä¸ªéœ€è¦ç¿»è¯‘çš„æ–‡æœ¬é¡¹ï¼Œæ€»å­—ç¬¦æ•°: {total_character_count}")
    
    if not text_items:
        print("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ï¼Œç›´æ¥ä¿å­˜")
        prs.save(output_path)
        return {
            "translated_file_path": output_path,
            "token_count": 0,
            "character_count": 0
        }
    
    # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡ç¿»è¯‘æ‰€æœ‰æ–‡æœ¬
    print(f"å¼€å§‹æ‰¹é‡ç¿»è¯‘ {len(text_items)} ä¸ªæ–‡æœ¬...")
    all_texts = [item['text'] for item in text_items]
    
    try:
        # ä½¿ç”¨æ‰¹é‡ç¿»è¯‘ï¼Œå¤§å¹…å‡å°‘APIè°ƒç”¨æ¬¡æ•°
        # æœ¯è¯­å‰/åå¤„ç†
        try:
            db = SessionLocal()
            options = get_terminology_options(db)
            if options.get("terminology_enabled", True):
                cat_ids = kwargs.get("category_ids", None)
                if options.get("categories_enabled", True):
                    if not cat_ids or (isinstance(cat_ids, list) and len(cat_ids) == 0):
                        processed_texts, mappings = all_texts, [{} for _ in all_texts]
                    else:
                        processed_texts, mappings = preprocess_texts_with_categories(
                            db, all_texts, src_lang, tgt_lang, cat_ids,
                            case_sensitive=bool(options.get("case_sensitive", False)), user_id=user_id
                        )
                else:
                    processed_texts, mappings = preprocess_texts(
                        db, all_texts, src_lang, tgt_lang,
                        case_sensitive=bool(options.get("case_sensitive", False)), user_id=user_id
                    )
            else:
                processed_texts, mappings = all_texts, [{} for _ in all_texts]
        finally:
            try:
                db.close()
            except Exception:
                pass

        translated_texts, batch_token_count = translate_batch(processed_texts, src_lang, tgt_lang, engine=engine)
        total_token_count = batch_token_count
        
        if len(translated_texts) != len(all_texts):
            print(f"è­¦å‘Šï¼šç¿»è¯‘ç»“æœæ•°é‡ä¸åŒ¹é…ï¼ŒæœŸæœ› {len(all_texts)}ï¼Œå®é™… {len(translated_texts)}")
            # å¦‚æœæ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨åŸæ–‡å¡«å……
            while len(translated_texts) < len(all_texts):
                translated_texts.append(all_texts[len(translated_texts)])
        
        # æœ¯è¯­åå¤„ç†
        if options.get("terminology_enabled", True):
            translated_texts = postprocess_texts(translated_texts, mappings)

        print(f"æ‰¹é‡ç¿»è¯‘å®Œæˆï¼Œæ¶ˆè€— {total_token_count} tokens")
        
    except Exception as e:
        print(f"æ‰¹é‡ç¿»è¯‘å¤±è´¥: {e}")
        # ç›´æ¥è¿”å›å¤±è´¥ï¼Œä¸è¿›è¡Œé€æ®µç¿»è¯‘å›é€€
        print("ç¿»è¯‘å¤±è´¥ï¼Œè¿”å›é”™è¯¯")
        raise e
    
    # ç¬¬ä¸‰æ­¥ï¼šå°†ç¿»è¯‘ç»“æœå†™å›PPTX
    print("å¼€å§‹å°†ç¿»è¯‘ç»“æœå†™å›PPTX...")
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"è°ƒè¯•ä¿¡æ¯ï¼š")
    print(f"  - åŸå§‹æ–‡æœ¬æ•°é‡: {len(all_texts)}")
    print(f"  - ç¿»è¯‘ç»“æœæ•°é‡: {len(translated_texts)}")
    print(f"  - å‰5ä¸ªåŸå§‹æ–‡æœ¬: {all_texts[:5]}")
    print(f"  - å‰5ä¸ªç¿»è¯‘ç»“æœ: {translated_texts[:5]}")
    
    # ç»Ÿè®¡ç¿»è¯‘ç»“æœ
    translated_count = 0
    untranslated_count = 0
    
    for i, item in enumerate(text_items):
        if i < len(translated_texts):
            translated_text = translated_texts[i]
            original_text = item['text']
            
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
            if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                print(f"è°ƒè¯• - é¡¹ç›® {i}:")
                print(f"  åŸå§‹æ–‡æœ¬: '{original_text}'")
                print(f"  ç¿»è¯‘ç»“æœ: '{translated_text}'")
                print(f"  ç±»å‹: {type(translated_text)}")
                print(f"  æ˜¯å¦ç›¸ç­‰: {translated_text == original_text}")
            
            # æ£€æŸ¥ç¿»è¯‘ç»“æœçš„æœ‰æ•ˆæ€§
            if translated_text is None or not isinstance(translated_text, str):
                print(f"è­¦å‘Š: æ–‡æœ¬ '{original_text[:50]}...' çš„ç¿»è¯‘ç»“æœæ— æ•ˆï¼Œä½¿ç”¨åŸæ–‡")
                translated_text = original_text
            elif translated_text.strip() == "":
                print(f"è­¦å‘Š: æ–‡æœ¬ '{original_text[:50]}...' çš„ç¿»è¯‘ç»“æœä¸ºç©ºï¼Œä½¿ç”¨åŸæ–‡")
                translated_text = original_text
            else:
                translated_text = _normalize_text(translated_text)
            
            # æ£€æŸ¥æ˜¯å¦çœŸçš„è¢«ç¿»è¯‘äº†ï¼ˆä¸æ˜¯åŸæ–‡ï¼‰
            if translated_text != original_text:
                translated_count += 1
                if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                    print(f"  âœ… ç¿»è¯‘æˆåŠŸ: '{original_text[:30]}...' -> '{translated_text[:30]}...'")
            else:
                untranslated_count += 1
                if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                    print(f"  âŒ ç¿»è¯‘å¤±è´¥æˆ–æœªå˜åŒ–: '{original_text[:30]}...'")
            
            # ç¡®ä¿translated_textæ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²
            if not isinstance(translated_text, str):
                translated_text = str(translated_text) if translated_text is not None else original_text
            
            if item['type'] == 'shape':
                paragraph = item.get('paragraph') or prs.slides[item['slide_idx']].shapes[item['shape_idx']].text_frame.paragraphs[item['para_idx']]
                # å°½é‡ä¿ç•™æ ·å¼ï¼šä¿ç•™ç¬¬ä¸€ä¸ª runï¼Œå†™å…¥è¯‘æ–‡ï¼Œæ¸…ç©ºå…¶ä½™ run æ–‡æœ¬
                if paragraph.runs:
                    paragraph.runs[0].text = translated_text
                    # å…¶ä½™ run æ¸…ç©ºæ–‡æœ¬ä½†ä¸ç§»é™¤ï¼Œä¿ç•™æ ·å¼
                    for r in paragraph.runs[1:]:
                        r.text = ""
                else:
                    paragraph.add_run().text = translated_text
                
            elif item['type'] == 'table':
                paragraph = item.get('paragraph')
                if paragraph is None:
                    slide = prs.slides[item['slide_idx']]
                    shape = slide.shapes[item['shape_idx']]
                    cell = shape.table.rows[item['row_idx']].cells[item['col_idx']]
                    paragraph = cell.text_frame.paragraphs[item['para_idx']]
                if paragraph.runs:
                    paragraph.runs[0].text = translated_text
                    for r in paragraph.runs[1:]:
                        r.text = ""
                else:
                    paragraph.add_run().text = translated_text
                
            elif item['type'] == 'notes':
                paragraph = item.get('paragraph') or prs.slides[item['slide_idx']].notes_slide.notes_text_frame.paragraphs[item['para_idx']]
                if paragraph.runs:
                    paragraph.runs[0].text = translated_text
                    for r in paragraph.runs[1:]:
                        r.text = ""
                else:
                    paragraph.add_run().text = translated_text
    
    # Save the translated presentation
    print("ä¿å­˜ç¿»è¯‘åçš„PPTX...")
    prs.save(output_path)
    
    # è¾“å‡ºè¯¦ç»†çš„ç¿»è¯‘ç»Ÿè®¡
    print(f"âœ… PPTXç¿»è¯‘å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"ğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»æ–‡æœ¬æ•°: {len(text_items)}")
    print(f"   - å·²ç¿»è¯‘æ•°: {translated_count}")
    print(f"   - æœªç¿»è¯‘æ•°: {untranslated_count}")
    print(f"   - ç¿»è¯‘å®Œæˆç‡: {(translated_count/len(text_items)*100):.1f}%")
    print(f"   - æ¶ˆè€— {total_token_count} tokens")
    print(f"   - å¤„ç† {total_character_count} å­—ç¬¦")
    
    # Return metadata
    return {
        "translated_file_path": output_path,
        "token_count": total_token_count,
        "character_count": total_character_count,
        "total_texts": len(text_items),
        "translated_texts": translated_count,
        "untranslated_texts": untranslated_count,
        "translation_rate": (translated_count/len(text_items)*100)
    }

def main():
    if len(sys.argv) < 5:
        print("Usage: python translator_pptx_direct.py input.pptx output.pptx src_lang tgt_lang [engine] [max_workers]")
        sys.exit(1)
    inp = sys.argv[1]
    outp = sys.argv[2]
    src = sys.argv[3]
    tgt = sys.argv[4]
    engine = sys.argv[5] if len(sys.argv) > 5 else "deepseek"
    max_workers = int(sys.argv[6]) if len(sys.argv) > 6 else 4

    try:
        translate_pptx(inp, outp, src, tgt, engine=engine, max_workers=max_workers)
        print(f"âœ… Translation finished. Output: {outp}")
    except Exception:
        print("âŒ Translation failed:")
        traceback.print_exc()
        sys.exit(2)


if __name__ == "__main__":
    main()
